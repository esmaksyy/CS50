Times:

10 simulations: (0m0,027s)
100 simulations: (0m0,041s)
1000 simulations: (0m0,035s)
10000 simulations: (0m0,093s)
100000 simulations: (0m0,735s)
1000000 simulations: (0m6,388s)

Questions:

Which predictions, if any, proved incorrect as you increased the number of simulations?: Based on the data, it appears that the predictions became more accurate as the number of simulations increased. The probabilities of the teams winning became more stable and converged towards their true values.

Suppose you're charged a fee for each second of compute time your program uses.
After how many simulations would you call the predictions "good enough"?: The point at which predictions can be considered "good enough" depends on the specific application and its requirements. In this case, one might consider the predictions "good enough" after a certain number of simulations when the changes in probabilities become negligible and the cost of additional simulations outweighs the marginal benefit in accuracy. For example, if the cost of compute time is a significant factor, you might call the predictions "good enough" after 100,000 simulations, as it provides a reasonably accurate result without incurring excessive costs. However, the exact threshold would depend on the context and the trade-off between accuracy and cost.